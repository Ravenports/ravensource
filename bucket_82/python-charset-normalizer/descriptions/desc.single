
<h1 align="center">Charset Detection, for Everyone ğŸ‘‹ [image]</h1>

<p align="center">
  <sup>The Real First Universal Charset Detector</sup><br>
  
    [image]
  
  
      [image]
  
  
    [image]
  
</p>

> A library that helps you read text from an unknown charset encoding.<br
/> Motivated by `chardet`,
> I'm trying to resolve the issue by taking a new approach.
> All IANA character set names for which the Python core library provides
codecs are supported.

<p align="center">
  >>>>> ğŸ‘‰ Try Me Online Now, Then Adopt Me ğŸ‘ˆ  <<<<<
</p>

This project offers you an alternative to **Universal Charset Encoding
Detector**, also known as **Chardet**.

| Feature       | [Chardet]       | Charset Normalizer | [cChardet] |
| ------------- | :-------------: | :------------------: |
:------------------: |
| `Fast`         | âŒ<br>          | âœ…<br>             | âœ… <br> |
| `Universal**`     | âŒ            | âœ…                 | âŒ |
| `Reliable` **without** distinguishable standards | âŒ | âœ… | âœ… |
| `Reliable` **with** distinguishable standards | âœ… | âœ… | âœ… |
| `Free & Open`  | âœ…             | âœ…                | âœ… |
| `License` | LGPL-2.1 | MIT | MPL-1.1
| `Native Python` | âœ… | âœ… | âŒ |
| `Detect spoken language` | âŒ | âœ… | N/A |
| `Supported Encoding` | 30 | :tada: [93]  | 40

<p align="center">
[image][image]

*\*\* : They are clearly using specific code for a specific encoding even
if covering most of used one*<br> 

## â­ Your support

*Fork, test-it, star-it, submit your ideas! We do listen.*
  
## âš¡ Performance

This package offer better performance than its counterpart Chardet. Here
are some numbers.

| Package       | Accuracy       | Mean per file (ms) | File per sec (est)
|
| ------------- | :-------------: | :------------------: |
:------------------: |
|      [chardet]        |     92 %     |     220 ms      |       5 file/sec
       |
| charset-normalizer |    **98 %**     |     **40 ms**      |       25
file/sec    |

| Package       | 99th percentile       | 95th percentile | 50th percentile
|
| ------------- | :-------------: | :------------------: |
:------------------: |
|      [chardet]        |     1115 ms     |     300 ms      |       27 ms  
     |
| charset-normalizer |    460 ms     |     240 ms      |       18 ms    |

Chardet's performance on larger file (1MB+) are very poor. Expect huge
difference on large payload.

> Stats are generated using 400+ files using default parameters. More
details on used files, see GHA workflows.
> And yes, these results might change at any time. The dataset can be
updated to include more files.
> The actual delays heavily depends on your CPU capabilities. The factors
should remain the same.

[cchardet] is a non-native (cpp binding) and unmaintained faster
alternative with 
a better accuracy than chardet but lower than this package. If speed is the
most important factor, you should try it.

## âœ¨ Installation

Using PyPi for latest stable
```sh
pip install charset-normalizer -U
```

If you want a more up-to-date `unicodedata` than the one available in your
Python setup.
```sh
pip install charset-normalizer[unicode_backport] -U
